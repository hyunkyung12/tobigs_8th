예측률 높이기



from tensorflow.examples.tutorials.mnist import input_data



mnist = input_data.read_data_sets("/tmp/data/")



X_train = mnist.train.images 

X_test = mnist.test.images

y_train = mnist.train.labels.astype("int")

y_test = mnist.test.labels.astype("int")



reset_graph()



X = tf.placeholder(tf.float32, shape=(None, n_inputs), name="X")

y = tf.placeholder(tf.int64, shape=(None), name="y")



# 노드 수 바꿔보기, 레이어 수 바꿔보기

n_inputs = 50*50  

n_hidden1 = 300

n_hidden2 = 200

n_hidden3 = 100

n_hidden4 = 50

n_outputs = 10



training = tf.placeholder_with_default(False, shape=(), name='training')



# dropout 

dropout_rate = 0.5

X_drop = tf.layers.dropout(X, dropout_rate, training=training)





with tf.name_scope("dnn"):

    hidden1 = tf.layers.dense(X_drop, n_hidden1, name="hidden1",

                              activation=tf.nn.relu)

    hidden1_drop = tf.layers.dropout(hidden1, dropout_rate, training=training)

    hidden2 = tf.layers.dense(hidden1, n_hidden2, name="hidden2",

                              activation=tf.nn.relu)

    hidden2_drop = tf.layers.dropout(hidden2, dropout_rate, training=training)

    hidden3 = tf.layers.dense(hidden1, n_hidden3, name="hidden3",

                              activation=tf.nn.relu)

    hidden3_drop = tf.layers.dropout(hidden3, dropout_rate, training=training)

    hidden4 = tf.layers.dense(hidden1, n_hidden4, name="hidden4",

                              activation=tf.nn.relu)

    hidden4_drop = tf.layers.dropout(hidden4, dropout_rate, training=training)    

    logits = tf.layers.dense(hidden4_drop, n_outputs, name="outputs")



with tf.name_scope("loss"):

    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)

    loss = tf.reduce_mean(xentropy, name="loss")



# learning rate 바꿔보기

learning_rate = 0.001



# optimizer 바꿔보기

with tf.name_scope("train"):

    optimizer = tf.train.AdamOptimizer(learning_rate)

    training_op = optimizer.minimize(loss) # 최종적으로 필요한 함수



with tf.name_scope("eval"):

    correct = tf.nn.in_top_k(logits, y, 1) # 10개중에 가장 높게 나온것을 y와 비교 => 1이면 참 0이면 거짓

    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32)) # 평균내면 정확도



init = tf.global_variables_initializer()

saver = tf.train.Saver()



# epoch 과 batch 수 바꿔보기

n_epochs = 100

n_batches = 70 



with tf.Session() as sess:

    init.run()

    for epoch in range(n_epochs):

        for iteration in range(mnist.train.num_examples // batch_size):

            X_batch, y_batch = mnist.train.next_batch(batch_size)

            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})

        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})

        acc_test = accuracy.eval(feed_dict={X: mnist.test.images, y: mnist.test.labels})

        print(epoch, "Train accuracy:", acc_train, "Test accuracy:", acc_test)



    save_path = saver.save(sess, "C:/Users/USER/Desktop/tobigs/tobigs/tmp/my_model_final.ckpt")
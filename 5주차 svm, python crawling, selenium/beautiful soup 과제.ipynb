import requests

from bs4 import BeautifulSoup

import pandas as pd

import csv



# 글자 정리하는 함수

def clean(var):

    var = var.replace("\n","")

    var = var.replace("\t","")

    return(var)



# 입력값으로 초기url을 받음

# 여러페이지를 크롤링하므로 page넘버는 비우고 입력



def crawling(url1):

    page1 = []

    for i in ["1","2","3","4","5"] :

        r = requests.get(url1+i)

        s = BeautifulSoup(r.content,"html.parser")

        page1.append(s)



    all = []

    sub = []

    for i in range(len(page1)):

        all1 = page1[i].find("tbody",{"id":"au_board_list"})

        title_all = all1.findAll("td",{"class":"title"})

        field_all = all1.findAll("td",{"class":"field"})

        question_all = all1.findAll("td",{"class":"questioner"})

        sub_all = all1.findAll("a",{"rel":"KIN"})

        all.append([title_all,field_all,question_all])

        sub.append(sub_all)



    result = []

    for i in (0,1,2,3,4):

        for k in range(len(title_all)):

            title = all[i][0][k].text

            field = all[i][1][k].text

            questioner = all[i][2][k].text

            title = clean(title)

            field = clean(field)

            questioner = clean(questioner)

            result.append([title,field,questioner])



    url = "http://kin.naver.com"

    suburl = []

    for i in (0,1,2,3,4):

        for j in range(len(title_all)):

            subsub = url+sub[i][j]['href']

            suburl.append(subsub)



    page2 = []

    for i in range(len(suburl)) :

        r = requests.get(suburl[i])

        s = BeautifulSoup(r.content,"html.parser")

        a = s.findAll("div",{"class":"_endContentsText"})

        page2.append(a)



    result2 = []

    for i in range(len(page2)):

        question = page2[i][0].text

        question = clean(question)

        answer = page2[i][1].text

        answer = clean(answer)

        result2.append([question,answer])



    df = pd.DataFrame(result,columns=["title","field","questioner"])

    df2 = pd.DataFrame(result2,columns=["question","answer"])



    df['question']=df2['question']

    df['answer']=df2['answer']

    

    return(df)



# 세 분야의 결과값

df_1 = crawling("http://kin.naver.com/qna/list.nhn?m=expertAnswer&dirId=60201&queryTime=2017-08-21%2004%3A06%3A21&page=")

df_2 = crawling("http://kin.naver.com/qna/list.nhn?m=expertAnswer&dirId=60202&queryTime=2017-08-21%2003%3A56%3A28&page=")

df_3 = crawling("http://kin.naver.com/qna/list.nhn?m=expertAnswer&dirId=60203&queryTime=2017-08-21%2004%3A06%3A47&page=")

df = df_1.append(df_2).append(df_3)



# 저장

df.to_csv("crawling.csv")


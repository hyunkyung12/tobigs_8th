#tree control
?tree.control
tree.control = list(mincut = 1, minsize = 3, nmax = 300) # 가독성을 위해 controll을 따로 만들어쑴
treemod2 = tree(AHD~.,data=train,control = tree.control)
treemod2 = tree(AHD~.,data=train,control = list(mincut = 1, minsize = 3, nmax = 300))
#두가지 방법 모두 가능.
#가독성을 위해 list로 따로 만들어서.
treemod2
plot(treemod2)
text(treemod2)
#가지치기
#k-fold Cross-validation 방법을 사용해서 train셋을 여러번 쪼개서 테스트 한 다음
#분산이 가장 낮은 가지의 수를 선택하면 됩니다.
#tree패키지에서 제공하는 cv.tree 함수를 사용하면 간단하게 확인할 수 있습니다.
tree.prune = cv.tree(treemod,FUN=prune.misclass)
plot(tree.prune)# 분산이 가장 낮은 곳을 찾으세요. 이경우에는 6
# 잘못 분류될 확률, 6일때가 가장 낮음
# for regression decision tree, use prune.tree function
tree.prune2 = prune.misclass(treemod, best=6)
plot.new()
par(mfrow=c(1,2))
plot(treemod);text(treemod)
plot(tree.prune2);text(tree.prune2)
#install.packages("caret")
library(caret)
tree_pred = predict(tree.prune2,newdata=test)
head(tree_pred)
tree_pred = predict(tree.prune2,newdata=test,type="class") #class를 정해서 반환함
head(tree_pred)
confusionMatrix(tree_pred,test$AHD)
#Accuracy : 0.7303
#여기서 cut-off를 수정할 수 있다. 일반적으로 0.5를 기준으로 하지만
#경우에 따라서 데이터의 비율로 할 수 있다.
#이 경우에는 NO를 기준으로 하였다. 그렇다면 원 data의 no 비율인 0.53 사용
tree_pred2 = predict(tree.prune2,newdata=test)
tree_pred2 = as.factor(tree_pred2[,1]>0.53) #0.53 은 그냥 임의의 값
levels(tree_pred2) = c("Yes","No")
tree_pred2
confusionMatrix(tree_pred2,test$AHD)
#Accuracy : 0.7303
##2. rpart package
install.packages("rpart")
library(rpart)
rpart_tree = rpart(AHD~., data= train)
par(mfrow=c(1,1))
plot(rpart_tree)
text(rpart_tree)
#rpart control
?rpart.control
rpart.control= list(minsplit = 30, cp = 0.01, maxdepth = 2)
rpart_tree2 = rpart(AHD~., data= train,control = rpart.control)
plot.new()
par(mfrow=c(1,2))
plot(rpart_tree);text(rpart_tree)
plot(rpart_tree2);text(rpart_tree2) # 최소인것을 기준으로 pruning
#에 rpart패키지도 과적합화 문제가 있기 때문에 가지치기(Pruning)을 해 보도록 하겠습니다.
#rpart패키지에서는 cv.tree와 유사하게 cross-validation을 계산해 주는 함수로
#print.cp를 제공하고 있습니다.
par(mfrow=c(1,1))
printcp(rpart_tree)
plotcp(rpart_tree)
#xerror가 가장 낮은 size of tree를 선택
rpart.prune= prune(rpart_tree,cp=rpart_tree$cptable[which.min(rpart_tree$cptable[,"xerror"])],"CP")
plot(rpart.prune)
text(rpart.prune)
#predcit
rpart_pred = predict(rpart.prune, newdata= test, type="class")
confusionMatrix(rpart_pred, test$AHD)
#Accuracy : 0.7303
##3. Party pacakage
#install.packages("party")
library(party)
party_tree = ctree(AHD~.,data=train)
plot(party_tree)
#ctree control
?ctree_control
party_tree2 = ctree(AHD~.,data=train,controls = ctree_control(minsplit = 20,mtry = 0, maxdepth = 1)) # list가 적용이 안되어 함수안에 그냥 써줌
plot(party_tree2)
#party패키지는 가지치기를 significance를 사용해서 하기 때문에
#별도의 pruning 과정이 필요 없습니다.
party_pred = predict(party_tree2, newdata= test, type="response") #type ="respose"가 다른점!
confusionMatrix(party_pred, test$AHD)
#Accuracy : 0.7303
install.packages("randomForest")
library(randomForest)
ranfo = randomForest(AHD~.,importance=TRUE,data=train)
#ntree : Number of trees to grow
#mtry	 : Number of variables randomly sampled as candidates at each split.
#Note that the default values are different for classification
#(sqrt(p) where p is number of variables in x) and regression (p/3)
#replace: Should sampling of cases be done with or without replacement?
#cutoff : (Classification only) A vector of length equal to number of classes.
#sampsize	: Size(s) of sample to draw.
?randomForest
ranfo2 = randomForest(AHD~.,importance=TRUE,data=train, ntree=10)
importance(ranfo)
varImpPlot(ranfo) # 위에있는 것들 순으로 중요도가 높은것
ranfo_pred = predict(ranfo, newdata = test)
confusionMatrix(ranfo_pred, test$AHD)
#Accuracy : 0.8202
#MeanDecreaseAccuracy : 예측에 대한 정확성의 평균 감소
#MeanDecreaseAccuracy : Gini 감소에 대한 평균 감소
install.packages("rpart")
rm(list=ls())
install.packages("ipred")
install.packages("doBy")
library(ipred)
library(doBy)
rm(list=ls())
set.seed(0810)
library(ipred)
library(doBy)
ind<-1:nrow(iris)
data1 <- cbind(iris,ind)
data1
test <- data1[!data1$ind %in% train$ind,]
train <- sampleBy(Species~.,frac=0.7,replace=FALSE,data=data1) # 타켓 변수의 비율을 동일하게 조정
test <- data1[!data1$ind %in% train$ind,]
train <- train[,-6]
test <- test[,-6]
summary(train$Species)
summary(test$Species)
bagg_mod <- bagging(Species~.,data=train, nbagg=50, coob=TRUE)
bagg_mod <- bagging(Species~.,data=train, nbagg=50, coob=TRUE)
bagg_mod$mtrees[[1]] # 각 부트스트랩 샘플로 키워진 나무들
bagg_mod$err # out-of-bag에 대한 예측의 오분류율
bagg_pred<-predict(bagg_mod,test[,-5])
table(bagg_pred, test[,5])
install.packages("adabag")
library(adabag)
boost_mod<-boosting(Species~., data=train, mfinal=10, control=rpart.control(maxdepth = 5))
boost_mod$weights # 각 나무의 오류율로 각 나무별 가중치
boost_mod$votes[1:10,] # 나무들의 투표
boost_mod$importance # 변수별 중요도
boost_pred<-predict(boost_mod, newdata=test[,-5], type="class") # factor형이기 때문에 class를 씀
table(boost_pred$class, test[,5])
library(caret)
library(caretEnsemble)
library(pROC)
library(mlbench)
install.packages("caretEnsemble")
library(caretEsnemble)
library(caretEsnemble)
install.packages("caretEsnemble")
library(caretEsnemble)
install.packages("caretEsnemble")
library(caretEsnemble)
data(Sonar)
idx <- createDataPartition(y= Sonar$Class, p=0.7, list=FALSE)
train <- Sonar[idx,]
test <- Sonar[-idx,]
fitControl <- trainControl(## 10-fold CV
method = "repeatedcv",
number = 10,
## repeated ten times
repeats = 10)
gbmFit1 <- train(Class ~ ., data = train,
method = "gbm",
trControl = fitControl,
verbose = FALSE)
install.packages("gbm")
library(gbm)
gbmFit1 <- train(Class ~ ., data = train,
method = "gbm",
trControl = fitControl,
verbose = FALSE)
library(gbm)
gbmFit1 <- train(Class ~ ., data = train,
method = "gbm",
trControl = fitControl,
verbose = FALSE)
fit_treebag <- train(Class~., data=train, method="treebag", metric="Accuracy", trControl=fitControl)
install.packages("h2o")
library(h2o)
h2o.init() #h2o 시작
train <- h2o.importFile("https://s3.amazonaws.com/erin-data/higgs/higgs_train_10k.csv")
test <- h2o.importFile("https://s3.amazonaws.com/erin-data/higgs/higgs_test_5k.csv")
dim(train)
dim(test)
y <- "response"
x <- names(train)[-1]
train[,y] <- as.factor(train[,y])
test[,y] <- as.factor(test[,y])
nfolds <- 10
my_gbm <- h2o.gbm(x = x,
y = y,
training_frame = train,
distribution = "bernoulli",
ntrees = 10, #트리갯수
max_depth = 3, #깊이 최대값
min_rows = 2, #leaf에 허용되는 관측치..?
learn_rate = 0.2, #학습률
nfolds = nfolds, #10-folds
fold_assignment = "Modulo", #AUTO, Random, Modulo, Stratified
keep_cross_validation_predictions = TRUE, #CV 값 유지
seed = 1)
my_rf <- h2o.randomForest(x = x,
y = y,
training_frame = train,
ntrees = 50,
nfolds = nfolds,
fold_assignment = "Modulo",
keep_cross_validation_predictions = TRUE,
seed = 1)
ensemble <- h2o.stackedEnsemble(x = x,
y = y,
training_frame = train,
model_id = "my_ensemble_binomial",#NULL로 해도 됨
base_models = list(my_gbm@model_id, my_rf@model_id))
perf <- h2o.performance(ensemble, newdata = test)
perf_gbm_test <- h2o.performance(my_gbm, newdata = test)
perf_rf_test <- h2o.performance(my_rf, newdata = test)
best_auc_test <- max(h2o.auc(perf_gbm_test), h2o.auc(perf_rf_test))
ensemble_auc_test <- h2o.auc(perf)
print(sprintf("Best Base-learner Test AUC:  %s", best_auc_test))
print(sprintf("Ensemble Test AUC:  %s", ensemble_auc_test))
learn_rate_opt <- c(0.01, 0.03) #learning rate
max_depth_opt <- c(3, 4, 5, 6, 9) #max depth
sample_rate_opt <- c(0.7, 0.8, 0.9, 1.0) #default 1
col_sample_rate_opt <- c(0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8) #default 1
hyper_params <- list(learn_rate = learn_rate_opt,
max_depth = max_depth_opt,
sample_rate = sample_rate_opt, #row sample rate per tree
col_sample_rate = col_sample_rate_opt) #column sample rate
search_criteria <- list(strategy = "RandomDiscrete", #모두를 임의로 검색
max_models = 3,#max_model or max_runtime_secs로 정지 기준
seed = 1)
gbm_grid <- h2o.grid(algorithm = "gbm",
grid_id = "gbm_grid_binomial",
x = x,
y = y,
training_frame = train,
ntrees = 10,
seed = 1,
nfolds = nfolds,
fold_assignment = "Modulo",
keep_cross_validation_predictions = TRUE,
hyper_params = hyper_params,
search_criteria = search_criteria)
ensemble <- h2o.stackedEnsemble(x = x,
y = y,
training_frame = train,
model_id = "ensemble_gbm_grid_binomial",
base_models = gbm_grid@model_ids)
perf <- h2o.performance(ensemble, newdata = test)
getauc_fun <- function(m){
h2o.auc(h2o.performance(h2o.getModel(m), newdata = test))
}
baselearner_aucs <- sapply(gbm_grid@model_ids, getauc_fun)
baselearner_best_auc_test <- max(baselearner_aucs)
ensemble_auc_test <- h2o.auc(perf)
print(sprintf("Best Base-learner Test AUC:  %s", baselearner_best_auc_test))
print(sprintf("Ensemble Test AUC:  %s", ensemble_auc_test))
install.packages("drat", repos="https://cran.rstudio.com")
drat:::addRepo("dmlc")
install.packages("xgboost", repos="http://dmlc.ml/drat/", type = "source")
library(xgboost)
install.packages("xgboost", repos="http://dmlc.ml/drat/", type = "source")
library(xgboost)
data(agaricus.train, package='xgboost')
data(agaricus.test, package='xgboost')
train <- agaricus.train
install.packages("xgboost")
library(xgboost)
data(agaricus.train, package='xgboost')
data(agaricus.test, package='xgboost')
train <- agaricus.train
test <- agaricus.test
str(train)
train$data[1:10,]
str(train$data)
class(train$data)
colnames(train$data) # 126개 변수 이름
dim(train$data) # train datasms 6513개
str(train$label) # 6513개 라벨~~
class(train$label)
dim(test$data) #test data는 1611개
bstSparse <- xgboost(data = train$data, label = train$label, max.depth = 2, eta = 1, nthread = 2, nround = 2, objective = "binary:logistic")
bstDense <- xgboost(data = as.matrix(train$data), label = train$label, max.depth = 2, eta = 1, nthread = 2, nround = 2, objective = "binary:logistic")
dtrain <- xgb.DMatrix(data = train$data, label = train$label)
bstDMatrix <- xgboost(data = dtrain, max.depth = 2, eta = 1, nthread = 2, nround = 2, objective = "binary:logistic")
bst <- xgboost(data = dtrain, max.depth = 2, eta = 1, nthread = 2, nround = 2, objective = "binary:logistic", verbose = 0)
bst <- xgboost(data = dtrain, max.depth = 2, eta = 1, nthread = 2, nround = 2, objective = "binary:logistic", verbose = 1)
bst <- xgboost(data = dtrain, max.depth = 2, eta = 1, nthread = 2, nround = 2, objective = "binary:logistic", verbose = 2)
pred <- predict(bst, test$data)
length(pred)
length(test$label) ## pred와 test$label 갯수 같은지 확인
head(pred) # 0,1로 예측해야함
prediction <- as.numeric(pred > 0.5) # 0.5초과 -> 1   0.5 이하 -> 0
head(prediction)
table(prediction,test$label) #예측 결과 비교 1611개중 35개 틀림
mean(prediction != test$label) # 오차 2.1%
dtrain <- xgb.DMatrix(data = train$data, label=train$label)
dtest <- xgb.DMatrix(data = test$data, label=test$label)
watchlist <- list(train=dtrain, test=dtest)
bst <- xgb.train(data=dtrain, max.depth=2, eta=1, nthread = 2, nround=2, watchlist=watchlist, objective = "binary:logistic")
bst <- xgb.train(data=dtrain, max.depth=2, eta=1, nthread = 2, nround=2, watchlist=watchlist, eval.metric = "error", eval.metric = "logloss", objective = "binary:logistic")
setwd(getwd())
xgb.DMatrix.save(dtrain, "dtrain.buffer")#데이터, 파일 이름 순서
dtrain2 <- xgb.DMatrix("dtrain.buffer") # 불러와졌음.
bst <- xgb.train(data=dtrain2, max.depth=2, eta=1, nthread = 2, nround=2, watchlist=watchlist, objective = "binary:logistic")
dtest#DMatrix 데이터, 아무정보가 안뜸!
label = getinfo(dtest, "label")#test데이터의 label을 빼오기
pred <- predict(bst, dtest)
err <- as.numeric(sum(as.integer(pred > 0.5) != label))/length(label)
err # 에러는 2.1퍼
importance_matrix <- xgb.importance(model = bst)
importance_matrix
xgb.plot.importance(importance_matrix = importance_matrix)
xgb.dump(bst, with.stats = T)
xgb.plot.tree(model = bstDMatrix)
install.packages("DiagrammeR")
library(lars)
install.packages("lars")
library(lars)
data("diabetes")
rm(list=ls())
library(rgl)
setwd("C:/Users/USER/Desktop/투빅스/3주차 과제")
data = read.csv("2015_7차_직접측정 데이터.csv")
str(data)
mae <- function(actual, predict) {
if (length(actual) != length(predict)) {
message("실제값과 예측값의 길이가 다릅니다\n")
break
}
length <- length(actual)
errorSum <- sum(abs(actual - predict))
return(errorSum/length)
}
rmse <- function(actual, predict) {
if (length(actual) != length(predict)) {
message("실제값과 예측값의 길이가 다릅니다\n")
break
}
length <- length(actual)
errorSum <- sum((actual - predict)^2)
return(sqrt(errorSum/length))
}
data = data[,-which(names(data)=="목둘레")]
data = data[which(data$성별=="여"),]
data = data[,-2]
data = na.omit(data)
str(data)# 변수가 135 개이므로 차원축소가 필요해보임
set.seed(100)
index = sample(1:nrow(data),0.7*nrow(data),replace=F)
data_test = data[-index,-1]
data_train = data[index,-1]
test_label = data[-index,1]
train_label = data[index,1]
pca = prcomp(data_train,center=T,scale=T)
pca$rotation
plot(pca,type="l") #elbow point = 3 이므로 변수 3개만 선택
summary(pca)
train1 = train[,1:17]
fit1 = lm(label~.,data=train1)
summary(fit1)
library(car)
vif(fit1) # 1,2의 vif가 매우 높음
train2 = train[,1:19]
train2 = train2[,-c(2,3)] # vif가 높았던 것들을 제거하고 다시 16개 선택
fit2 = lm(label~.,data=train2)
summary(fit2)
vif(fit2)
fit_pred2 = predict(fit2,type="response",newdata=test)
test_pred2 = round(fit_pred2)
mae(test_label,test_pred2) # 1.95
rmse(test_label,test_pred2) # 2.61
rm(list=ls())
library(rgl)
setwd("C:/Users/USER/Desktop/투빅스/3주차 과제")
data = read.csv("2015_7차_직접측정 데이터.csv")
str(data)
#평균 절대 오차
mae <- function(actual, predict) {
if (length(actual) != length(predict)) {
message("실제값과 예측값의 길이가 다릅니다\n")
break
}
length <- length(actual)
errorSum <- sum(abs(actual - predict))
return(errorSum/length)
}
#평균 제곱근 오차
rmse <- function(actual, predict) {
if (length(actual) != length(predict)) {
message("실제값과 예측값의 길이가 다릅니다\n")
break
}
length <- length(actual)
errorSum <- sum((actual - predict)^2)
return(sqrt(errorSum/length))
}
## 1. 데이터 전처리
# 목둘레 변수 제거
data = data[,-which(names(data)=="목둘레")]
# 여자만 뽑기
data = data[which(data$성별=="여"),]
data = data[,-2]
# 결측치 제거
data = na.omit(data)
str(data)# 변수가 135 개이므로 차원축소가 필요해보임
## 2. test / train set 분리
set.seed(100)
index = sample(1:nrow(data),0.7*nrow(data),replace=F)
data_test = data[-index,-1]
data_train = data[index,-1]
test_label = data[-index,1]
train_label = data[index,1]
pca = prcomp(data_train,center=T,scale=T)
pca$rotation
## 3. 변수 줄이기
## 3-1. elbow point 를 이용해 변수를 줄이기
plot(pca,type="l") #elbow point = 3 이므로 변수 3개만 선택
trainprc = as.matrix(data_train) %*% pca$rotation
testprc = as.matrix(data_test) %*% pca$rotation
train = cbind(train_label,as.data.frame(trainprc))
test = cbind(test_label,as.data.frame(testprc))
colnames(train)[1] = "label"
colnames(test)[1] = "label"
str(train)
fit = lm(label~PC1+PC2+PC3,data=train)
summary(fit) # 모든 변수들이 유의함
fit_pred = predict(fit,type="response",newdata=test)
test_pred = round(fit_pred)
mae(test_label,test_pred) # 1.4
rmse(test_label,test_pred) # 2.04
## 3-2. summary의 cummulative proportion 을 이용해 변수 개수 정하기
summary(pca)
# 80이 넘어가는 변수 기준 => 16개 선택
train1 = train[,1:17]
fit1 = lm(label~.,data=train1)
summary(fit1)
library(car)
vif(fit1) # 1,2의 vif가 매우 높음
train2 = train[,1:19]
train2 = train2[,-c(2,3)] # vif가 높았던 것들을 제거하고 다시 16개 선택
fit2 = lm(label~.,data=train2)
summary(fit2)
vif(fit2)
fit_pred2 = predict(fit2,type="response",newdata=test)
test_pred2 = round(fit_pred2)
mae(test_label,test_pred2) # 1.95
rmse(test_label,test_pred2) # 2.61
# 90이 넘어가는 변수 기준 => 36개 선택
train3 = train[,1:37]
fit3 = lm(label~.,data=train3)
vif(fit3) # vif 가 10 이상인것들 제거 1,2,5,7,13,14,16
train4 = train[,1:44]
train4 = train3[,-c(2,3,6,8,14,15,17)] # vif가 높았던 것들을 제거하고 다시 36개 선택
fit4 = lm(label~.,data=train4)
vif(fit4)
summary(fit4)
fit_pred4 = predict(fit4,type="response",newdata=test)
test_pred4 = round(fit_pred4)
mae(test_label,test_pred4) # 2.29
rmse(test_label,test_pred4) # 2.98
## 3-3. vif 말고 변수선택의 방법으로 해보기
# 두개 같이 하면 오차가 커지는데 왜그러는지는 모르겠음 ㅠㅠ
fit.up = lm(label~.,data=train1) # train1 은 cummulative 가 80이 되던 지점
fit.low = lm(label~1,data=train1)
fit.forward = step(fit.low, list(lower=fit.low,upper=fit.up),direction="forward")
fit5 = lm(label ~ PC1 + PC2 + PC5 + PC3 + PC14 + PC7 + PC12 + PC8 + PC4 + PC16 + PC10 + PC6 + PC13 , data=train1)
summary(fit5)
fit_pred5 = predict(fit5,type="response",newdata=test)
test_pred5 = round(fit_pred5)
mae(test_label,test_pred5) # 0.87
rmse(test_label,test_pred5) # 1.48
fit.up1 = lm(label~.,data=train3) # train3 은 cumulative 가 90이 되던 지점
fit.low1 = lm(label~1,data=train3)
fit.forward1 = step(fit.low1, list(lower=fit.low1,upper=fit.up1),direction="forward")
fit6 = lm(label ~ PC1 + PC2 + PC5 + PC3 + PC27 + PC7 + PC12 + PC14 + PC8 + PC33 + PC4 + PC22 + PC16 + PC6 + PC10 + PC31 + PC34 + PC20 +
PC28 + PC21 + PC9 + PC25 + PC19 + PC29 + PC11 + PC17 + PC18 + PC35, data=train3)
summary(fit6)
fit_pred6 = predict(fit6,type="response",newdata=test)
test_pred6 = round(fit_pred6)
mae(test_label,test_pred6) # 0.84
rmse(test_label,test_pred6) # 1.46
## 최종 모델 ##
# 여성의 몸무게를 가장 잘 설명하는 모델은
# label ~ PC1 + PC2 + PC5 + PC3 + PC27 + PC7 + PC12 + PC14 + PC8 + PC33 + PC4 + PC22 + PC16 + PC6 + PC10 + PC31 + PC34 + PC20 +
# PC28 + PC21 + PC9 + PC25 + PC19 + PC29 + PC11 + PC17 + PC18 + PC35 이다
# fit5 와 fit6의 오차는 0.02 정도밖에 차이가 나지 않으므로 더 간단한
# label ~ PC1 + PC2 + PC5 + PC3 + PC14 + PC7 + PC12 + PC8 + PC4 + PC16 + PC10 + PC6 + PC13 을 선택하는것도 좋을것 같다
fit_pred1 = predict(fit1,type="response",newdata=test)
test_pred1 = round(fit_pred1)
mae(test_label,test_pred1) # 1.95
rmse(test_label,test_pred1) # 2.61
fit_pred3 = predict(fit3,type="response",newdata=test)
test_pred3 = round(fit_pred3)
mae(test_label,test_pred3) # 0.87
rmse(test_label,test_pred3) # 1.49
rm(list=ls())
gc()
df = read.csv("http://www-bcf.usc.edu/~gareth/ISL/Heart.csv")
str(df)
summary(df)
df = na.omit(df) # NA가 많으면 삭제말고 다른 방식을 선택
library(caret)
set.seed(1234)
intrain = createDataPartition(y=df$AHD, p=0.7, list=FALSE)
intrain
train = df[intrain, -1] # 라벨제거
test  = df[-intrain,-1]
table(train$AHD)/sum(table(train$AHD))
table(test$AHD)/sum(table(test$AHD))
library(tree)
treemod = tree(AHD~.,data=train)
treemod
plot(treemod)
text(treemod)
tree.control = list(mincut = 1, minsize = 3, nmax = 300) # 가독성을 위해 controll을 따로 만들어쑴

1) 전처리

import pickle

import pandas as pd

import sklearn

from sklearn.cross_validation import train_test_split



# 데이터 불러온 후 train / test 나누기

csv = pd.read_csv("ham_spam.csv")

train, test = sklearn.cross_validation.train_test_split(csv, train_size = 0.7)



# 데이터의 text와 category를 따로 저장, ham / spam => 1/0

def part(data):

    text = list(data.text)

    category = list(data.category)

    rate = [word.replace('ham','1') for word in category ]

    rate = [word.replace('spam','0') for word in rate ]

    return text,rate



train_part, train_rate = part(train)

test_part, test_rate = part(test)



# 길이가 3이하인 단어는 의미가 없는 관사들이 많아 3이상인 단어만 선택

def process(data): 

    text = [line.split(" ") for line in data]

    list = []

    for line in text:

        words = []

        for word in line:

            if len(word) >= 3:

                words.append(word)

        list.append(words)  

    return(list)



train_1 = process(train_part)

test_1 = process(test_part)



stop_words = ['you','are','also','and','that','the','about','which','for','only','your','was','his',

              'all','can','I\'m','You','It\'s','me','this','but','...']



# stop_words 거르기

def rm_stop(token):

    final = []

    for words in token:

        word_list = []

        for word in words:

            if word not in stop_words:

                word_list.append(word)

        final.append(word_list)

    return final



## 전처리가 끝난 모델

train_2 = rm_stop(train_1)

test_2 = rm_stop(test_1)



2) word2vec

import warnings

warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')



from gensim.models import Word2Vec

model = Word2Vec(train_2, size=100, window=10, min_count=10, workers=4)

w2v = dict(zip(model.wv.index2word, model.wv.syn0))



3) svm

from sklearn.feature_extraction.text import TfidfVectorizer

from collections import defaultdict

import numpy as np



# word2vec embedding vector와 word에 대한 tfidf 가중치를 이용한 vectorizer 함수

class TfidfEmbeddingVectorizer:

    def __init__(self, word2vec):

        self.word2vec = word2vec

        

    def transform(self, X):

        tfidf = TfidfVectorizer(analyzer = lambda x : x) 

        tfidf.fit(X)

        max_idf = max(tfidf.idf_) 

        word2weight = defaultdict(lambda : max_idf, [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()]) 

        

        array_list =[]

        for words in X:

            array_list.append(np.array(np.mean([self.word2vec[w]*word2weight[w] for w in words if w in self.word2vec] or [np.zeros(100)], axis = 0)))

        return(array_list)



vec_tf = TfidfEmbeddingVectorizer(w2v)

train_tf = vec_tf.transform(train_2)

test_tf = vec_tf.transform(test_2)



from sklearn.svm import SVC

clf = SVC(decision_function_shape='ovo')

svc_clf = clf.fit(train_tf, train_rate)



pred = svc_clf.predict(test_tf)

pred



from sklearn import metrics

print(metrics.classification_report(test_rate, pred))


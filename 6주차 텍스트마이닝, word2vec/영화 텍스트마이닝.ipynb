1) 크롤링

import requests

from bs4 import BeautifulSoup

import pandas as pd

import csv



# 크롤링 함수

def review_crawling(code):

    review_list = [] 



    for i in range(50):

        url = 'http://movie.naver.com/movie/bi/mi/pointWriteFormList.nhn?code='+code+'&type=after&isActualPointWriteExecute=false&isMileageSubscriptionAlready=false&isMileageSubscriptionReject=false&page={}'.format(i+1)

        webpage = requests.get(url)

        source = BeautifulSoup(webpage.content, 'html.parser', from_encoding='utf-8')

        reviews = source.findAll('p')[10:20]



        for review in reviews:

            review = review.get_text().strip().replace("\n","").replace("\t","")

            if "BEST" in review[0:4]:

                review = review.replace("BEST","")

            if "관람객" in review[0:3]:

                review = review.replace("관람객","")

            review_list.append(review)

    

    return(review_list)



review_list1 = review_crawling("152341") # 에나벨

review_list2 = review_crawling("152616") # 47미터

review_list3 = review_crawling("127338") # 애나벨1

review_list4 = review_crawling("92823") # 컨저링

review_list5 = review_crawling("156083") # 겟아웃

review_list6 = review_crawling("76309") # 플립

review_list7 = review_crawling("134963") # 라라랜드

review_list8 = review_crawling("38444") # 이터널선샤인

review_list9 = review_crawling("92075") # 어바웃타임

review_list10 = review_crawling("137915") # 미비포유



2) 각 문서에 대해 dtm 생성



from konlpy.tag import Twitter

from collections import Counter

from sklearn.feature_extraction.text import CountVectorizer



def dtm(review_list):

    

    # 품사 태깅

    twitter = Twitter()

    word_tag = [twitter.pos(line) for line in review_list] 



    # 명사만 추출

    noun_list = []

    for line in word_tag:

        for word,tag in line:

            if tag in ["Noun"]:

                if len(word)>=2: # 두글자 이상의 명사만 추출

                    noun_list.append(word)



    #3번이상 나온 단어들만 체크

    item = list(Counter(noun_list).most_common(50))

    noun = []

    for i in range(50):

        noun.append(item[i][0])



    Tf_dtm = CountVectorizer(min_df=1).fit(noun)

    # dataframe

    Tf_db = pd.DataFrame(Tf_dtm.transform(review_list).toarray(),columns=Tf_dtm.get_feature_names())

    return(Tf_db)



dtm1 = dtm(review_list1)

dtm2 = dtm(review_list2)

dtm3 = dtm(review_list3)

dtm4 = dtm(review_list4)

dtm5 = dtm(review_list5)

dtm6 = dtm(review_list6)

dtm7 = dtm(review_list7)

dtm8 = dtm(review_list8)

dtm9 = dtm(review_list9)

dtm10 = dtm(review_list10)



# 열개의 리뷰 공통으로 나오는 단어들이므로 특징을 파악할때 큰 의미가 없음

c1 = set(dtm1) & set(dtm2) & set(dtm3) & set(dtm4) & set(dtm5) & set(dtm6) & set(dtm7) & set(dtm8) & set(dtm9) & set(dtm10)



# 이정도는 공포/로맨스 를 구분해주는 단어라고 생각해서 남겨도 될것같음

c2 = set(dtm1) & set(dtm2) & set(dtm3) & set(dtm4) & set(dtm5)

c3 = set(dtm6) & set(dtm7) & set(dtm8) & set(dtm9) & set(dtm10)



# 열개 리뷰 공통단어 제외 => 각각 갯수는 동일

word1 = list(set(dtm1)-c1)

word2 = list(set(dtm2)-c1)

word3 = list(set(dtm3)-c1)

word4 = list(set(dtm4)-c1)

word5 = list(set(dtm5)-c1)

word6 = list(set(dtm6)-c1)

word7 = list(set(dtm7)-c1)

word8 = list(set(dtm8)-c1)

word9 = list(set(dtm9)-c1)

word10 = list(set(dtm10)-c1)



# list => string

str1 = '\n'.join(word1)

str2 = '\n'.join(word2)

str3 = '\n'.join(word3)

str4 = '\n'.join(word4)

str5 = '\n'.join(word5)

str6 = '\n'.join(word6)

str7 = '\n'.join(word7)

str8 = '\n'.join(word8)

str9 = '\n'.join(word9)

str10 = '\n'.join(word10)



word = [str1,str2,str3,str4,str5,str6,str7,str8,str9,str10]



3) Tf-idf

from sklearn.feature_extraction.text import TfidfVectorizer

from sklearn.metrics.pairwise import cosine_similarity

vectorizer = TfidfVectorizer()

X = vectorizer.fit_transform(word)

X = X.todense()

print(X)



4) 유사도 구하기

# 두 문서의 유사도를 구해주는 함수

def similarity(movie1_index, movie2_index):

    sim = cosine_similarity(X[movie1_index], X[movie2_index])

    return(float(sim))



# 빈 matrix

import numpy as np

a = np.zeros(shape=(10,10))



# 10X10 matrix 반환

for i in range(10):

    for j in range(10):

        a[i][j] = similarity(i,j)


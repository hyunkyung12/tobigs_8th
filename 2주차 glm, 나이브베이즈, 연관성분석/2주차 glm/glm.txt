일반화 선형모형 (glm)

1. 로지스틱 회귀분석 : 이산형 (0 or 1)

	- 생각과정
		분류    => 확률값 => 오즈    => 로짓
		0 or 1  => [0,1]  => [0,Inf] => (-Inf, Inf)

		오즈 = 성공확률 / 실패확률
		로짓 = log(오즈)

	- 회귀계수 구하기
		OLS 대신 MLE 이용 : 가능도함수를 최대로 하는 것을 찾는 것

	- p13의 p를 구하는 과정
		log(p/1-p) = ax+b
		p/(1-p) = exp(ax+b) : 승산
		p = (1-p)exp(ax+b)
		exp-p*exp = p

	- 회귀계수의 해석
		x가 한단위 변할때마다 y가 변화 : 일반 회귀
		x가 한단위 변할때마다 y의 로짓이 변화 => 오즈가 e의 베타만큼 변화 : 로지스틱 회귀
		e의 베타승 < 1 : 낮은 영향을 줌 , > 1 : 높은 영향을 줌

	- ROC Curve : 모델의 적합도를 볼 수 있음
 	- Sigmoid : 분류 관련 데이터들은 sigmoid 모양으로 생김

2. 소프트맥스 회귀분석 : 다범주

ex) 서울, 부산, 대구 => 서울 or not , 부산 or not , 대구 or not 으로 분류

서울 : 0.3 , 대구 : 0,2 , 부산 : 0.7 => 최대가 되는 것으로 분류 (여기서는 부산)

num of level(k) : 범주 수 (예제에서는 3) 

p 21
H(x) : 추정한 식
이진분류에 사용하는 경우 : y1*log(y1hat) + y2*log(y2hat)

3. 벌점회귀

다중공선성을 해결하는 경우
	1. ridge(능형회귀) : 행렬곱의 정의가 잘 안되는 경우 (샘플수 < 변수갯수)
		=> 베타계수를 축소 ? 최소제곱법에 penalty라는 파라미터를 추가해 계산
		penalty는 다양하게 걸 수 있음 ( ex. 베타<x^2 를 라그랑주 승수법으로 품 ? )
		능형회귀는 해석도 낮고, 예측도 높음
		람다 구하기 ? p26 에 보면 기울기가 꺾인 후 평평해지는 곳이 람다
		0에 가까이 작은 변수는 영향력이 작다 ( 작은수에 x를 곱한다고 생각 )
	2. lasso : 제곱 대신 절댓값 > covariance가 0으로 줄어드는 경우가 생김 > 모델에서 사라지는 변수 생김
	
4. cross validation

5. 과제
나이트 : 2간가로 1칸세로 / 1칸가로 2칸 세로

6. ROC Curve

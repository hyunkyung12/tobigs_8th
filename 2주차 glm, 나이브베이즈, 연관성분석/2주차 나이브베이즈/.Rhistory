a = 1
a
b = c(2,3)
b
system.time({
rm(list = ls())
my_function <- function(n)
{
count = 0
while(n!=1){
if(n%%2==0&&n%%3==0){
if(n%%2==0){
n = n/2
count = count+1
}else{
n = n/3
count = count+1
}else{
n = n-1
count = count+1
}
}
}
return(count)
}
my_function(sample(10000:100000, 1))
})
my_function <- function(n)
{
count = 0
while(n!=1){
if(n%3==0){
n = n/3
count=count+1
}else{
n = n-1
count=count+1}
if(n%2==0){
n = n/2
count=count+1
}else{
n = n-1
count=count+1}
}
return(count)
}
my_function <- function(n)
{
count = 0
while(n!=1){
if(n%3==0){
n = n/3
count=count+1
}else{
n = n-1
count=count+1}
if(n%2==0){
n = n/2
count=count+1
}else{
n = n-1
count=count+1}
}
return(count)
}
my_function <- function(n)
{
count = 0
while(n!=1){
if(n%%2==0&&n%%3==0){
if(n%%2==0){
n = n/2
count = count+1
}else{
n = n/3
count = count+1
}else{
n = n-1
count = count+1
}
}
}
return(count)
}
system.time({
rm(list = ls())
fibonacci_numaber <- function(i)
{
if(i==1){
return(0)
}else if(i==2){
return(1)
}else{
return(fibonacci_number(i-1)+fibonacci_number(i-2))
}
}
sum_of_fibonacci <- function(n)
{
sum = 0
for(j in 1:n){
sum = sum + fibonacci_number(j)
}
return(sum)
}
sum_of_fibonacci(sample(1000:10000, 1))
})
system.time({
rm(list = ls())
fibonacci_number <- function(i)
{
if(i==1){
return(0)
}else if(i==2){
return(1)
}else{
return(fibonacci_number(i-1)+fibonacci_number(i-2))
}
}
sum_of_fibonacci <- function(n)
{
sum = 0
for(j in 1:n){
sum = sum + fibonacci_number(j)
}
return(sum)
}
sum_of_fibonacci(sample(1000:10000, 1))
})
system.time({
rm(list = ls())
sum_of_fibonacci <- function(n)
{
a=0;b=1;sum=0
for(i in 1:n){
sum = sum+a
c = a+b;
a=b;
b=c;
}
return(sum%% 1000000009)
}
sum_of_fibonacci(sample(1000:10000, 1))
})
install.packages("caret")
install.packages("car")
install.packages("glmnet")
install.packages("nnet")
library(car)
library(glmnet)
library(caret)
library(caret)
setwd("C:/Users/USER/Desktop/glm")
digit <- read.csv("digit.csv")
digit$label <- as.factor(digit$label)
digit <- read.csv("digit.csv")
digit$label <- as.factor(digit$label)
set.seed(0127)
index <- sample(1:length(digit[,1]),length(digit[,1])*0.1,F)
train <- digit[index,]
test <- digit[-index,]
fit <- glm(label~.,data = train,family = binomial(link = "logit"))
reg.ridge <- cv.glmnet(x= data.matrix(train[,-1]),y= data.matrix(train[,1]),
family = "binomial", alpha = 0,nfold=10)
library(nnet)
reg.ridge <- cv.glmnet(x= data.matrix(train[,-1]),y= data.matrix(train[,1]),
family = "binomial", alpha = 0,nfold=10)
library(car)
library(glmnet)
library(caret)
reg.ridge <- cv.glmnet(x= data.matrix(train[,-1]),y= data.matrix(train[,1]),
family = "binomial", alpha = 0,nfold=10)
coef(reg.ridge)
plot(reg.ridge)
reg.ridge$lambda.min # cv error 최소화하는 lambda값
table(Actual= test[,1],Predicted = pred)
pred <- predict(reg.ridge,data.matrix(test[,-1]),s= reg.ridge$lambda.min,type="class")
table(Actual= test[,1],Predicted = pred)
confusionMatrix(pred,test[,1])
reg.ridge <- cv.glmnet(x= data.matrix(train[,-1]),y= data.matrix(train[,1]),
family = "binomial", alpha = 1,nfold=10)
coef(reg.ridge)
plot(reg.ridge)
reg.ridge <- cv.glmnet(x= data.matrix(train[,-1]),y= data.matrix(train[,1]),
family = "binomial", alpha = 1,nfold=10)
coef(reg.ridge)
reg.lasso <- cv.glmnet(x= data.matrix(train[,-1]),y= data.matrix(train[,1]),
family = "binomial", alpha = 1,nfold=10)
coef(reg.lasso)
plot(reg.lasso)
rm(list=ls())
library(arules)
library(arules)
install.packages("arules")
install.packages("arulesViz")
install.packages("arulesViz")
library(arules)
library(arulesViz)
data(Epub) #a lot of 0 : density of 0.001758755
summary(Epub)
class(Epub)
image(sample(Epub,500,replace=FALSE))
itemFrequencyPlot(Epub,support=0.01,main="item frequency plot support") #지지도 1% 이상의 item에 대한 막대그래프
Epub@itemInfo # $대신 @사용
Epub@data  #sparse matrix
Epub_rule1<-apriori(Epub, parameter=list(support=0.01,confidence=0.2,minlen=1))
계산함
# support가 너무 작으면 rule이 너무 많을수도 / 너무 적으면 rule이 너무 적을수도
Epub_rule1
Epub_rule2 <-apriori(Epub, parameter=list(support=0.001,confidence=0.2,minlen=1))
summary(Epub_rule2) #2개가 62개, 3개가 3개
Epub_rule2
inspect(Epub_rule2[1:20])
inspect(sort(Epub_rule2[1:20],by="lift"))
rule_interest <- subset(Epub_rule2, items %in% c("doc_72f", "doc_4ac"))
inspect(rule_interest)
library(arulesViz)
plot(Epub_rule2)
plot(sort(Epub_rlue2, by = "support"), method = "grouped")
plot(sort(Epub_rule2, by = "support"), method = "grouped")
plot(sort(Epub_rule2, by = "support")[1:20], method = "grouped")
plot(sort(Epub_rule2, by = "support"), method = "grouped")
plot(sort(Epub_rule2, by = "support")[1:20], method = "grouped")
plot(Epub_rule2)
plot(sort(Epub_rule2, by = "support"), method = "grouped")
plot(sort(Epub_rule2, by = "support")[1:20], method = "grouped")
plot(Epub_rule2,method="graph",interactive = T)
plot(Epub_rule2,method="paracoord")
data(Mushroom)
inspect(Mushroom[1:10])
itemFrequencyPlot(Mushroom, support=0.3)
itemFrequencyPlot(Mushroom, topN=20)
rule_poison<-apriori(Mushroom,parameter=list(support=0.25,confidence=0.8,minlen=1),appearance=list(rhs=c("Class=poisonous"),
default="lhs"))
summary(rule_poison)   #208rules
inspect(rule_poison)
inspect(sort(rule_poison,by='lift')[1:20])
matrix <- matrix(c(1,1,0,0,0,
1,0,1,0,0,
0,1,1,1,0,
1,0,0,0,1,
0,0,1,1,1), ncol=5, byrow=T)
dimnames(matrix)<-list(paste0("trans",1:5),letters[1:5])
trans.matrix <-as(matrix,"transactions")
trans.matrix
dimnames(matrix)
data(Adult)
install.packages("tm")
install.packages("e1071")
rm(list=ls())
setwd("C:/Users/USER/Desktop/투빅스/2주차 나이브베이즈/2주차 나이브베이즈")
sms_data <- read.csv("ham_spam.csv", stringsAsFactors = F) #변수 factor형으로 불러오지 않음 : F
str(sms_data) #type, text 두가지 변수
View(sms_data)
table(sms_data$type)
sms_data$type <- as.factor(sms_data$type) #type변수 factor형으로 변환
sms_data$text <- iconv(enc2utf8(sms_data$text), sub="byte") #UTF-8 encoding(오류 방지)
library(tm)
sms_corpus <- Corpus(VectorSource(sms_data$text)) #corpus 생성 : 단어를 다루는 자료형
print(sms_corpus)
sms_corpus[[1]]
sms_corpus[[1]]$content #내용
sms_corpus[[1]]$meta #정보
corpus_clean <- tm_map(sms_corpus, content_transformer(tolower)) #소문자로 통일
corpus_clean <- tm_map(corpus_clean, removeNumbers) #숫자 제거
corpus_clean <- tm_map(corpus_clean, removeWords, stopwords()) #기존에 존재하는 stopwords() 이용해 stopwords 제거
corpus_clean <- tm_map(corpus_clean, removePunctuation) #구두점 제거
corpus_clean <- tm_map(corpus_clean, stripWhitespace) #한칸 이상의 띄어쓰기 한칸으로 변환
sms_corpus[[1]]$content #전처리 전
corpus_clean[[1]]$content #전처리 후
sms_dtm <- DocumentTermMatrix(corpus_clean)
inspect(sms_dtm[1:10,1:10])
dim(sms_dtm) #서로 다른 8283개의 단어 존재
set.seed(0727)
index <- sample(1:dim(sms_data)[1],round(dim(sms_data)[1]*0.7))
sms_data_test <- sms_data[-index,]
sms_dtm_train <- sms_dtm[index,]
sms_dtm_test <- sms_dtm[-index,]
sms_corpus_train <- corpus_clean[index]
sms_corpus_test <- corpus_clean[-index]
table(sms_data_train$type)/table(sms_data_test$type) #type 비율 유사
set.seed(0727)
index <- sample(1:dim(sms_data)[1],round(dim(sms_data)[1]*0.7))
sms_data_train <- sms_data[index,]
sms_data_test <- sms_data[-index,]
sms_dtm_train <- sms_dtm[index,]
sms_dtm_test <- sms_dtm[-index,]
sms_corpus_train <- corpus_clean[index]
sms_corpus_test <- corpus_clean[-index]
table(sms_data_train$type)/table(sms_data_test$type) #type 비율 유사
sms_dict<- findFreqTerms(sms_dtm_train, 5) #빈도수 5이상인 단어만 저장
length(sms_dict) #1203개
sms_train <- DocumentTermMatrix(sms_corpus_train, list(dictionary = sms_dict)) #빈도수 5이상인 단어로 최종 dtm생성
sms_test <- DocumentTermMatrix(sms_corpus_test, list(dictionary = sms_dict))
convert_counts <- function(x){
x <- ifelse(x>0, 1, 0) #x가 0보다 크면 1 아니면 0
x <- factor(x, levels = c(0, 1), labels = c("No", "Yes"))
}
library(e1071)
sms_classifier <- naiveBayes(sms_train, sms_data_train$type)
sms_train <- apply(sms_train, 2, convert_counts) #column별 convert_counts 함수 적용
